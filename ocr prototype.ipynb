{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Bidirectional\n",
    "import numpy as np\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from demo import *\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_model = {'img15_480.jpg': ['Patch', 'Pan', 'holes', 'Sgns', 'Hhe', 'Bcs', 'put', 'NOF', 'UP'], 'img15_.jpg': ['Fou Reas', 'Patch', 'holes', 'signs', 'the', 'UP', 'Nof', 'put', 'F'], 'img15_352.jpg': ['Patch', 'holes', 'Rocds', 'Rougt R', 'the', 'put', 'signs', 'NoF', 'UP', 'pSi'], 'img15_858.jpg': ['Patch', 'Rouh Ross', 'holes', 'signs', 'the', 'UP', 'NOf', 'put'], 'img15_1280.jpg': ['Potch', 'Pou Ross', 'holes', 'signs', 'the', 'put', 'Nof', 'UP']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "texts=[]\n",
    "labels=[]\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "for x,y in ret_model.items():\n",
    "    labels.append(x)\n",
    "    texts.append(y)\n",
    "    length = len(y)\n",
    "    # if length>MAX_SEQUENCE_LENGTH:\n",
    "    #     MAX_SEQUENCE_LENGTH=length\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'holes': 1, 'put': 2, 'nof': 3, 'up': 4, 'patch': 5, 'signs': 6, 'the': 7, 'pan': 8, 'sgns': 9, 'hhe': 10, 'bcs': 11, 'fou reas': 12, 'f': 13, 'rocds': 14, 'rougt r': 15, 'psi': 16, 'rouh ross': 17, 'potch': 18, 'pou ross': 19}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_glove():\n",
    "    with open(\"../glove.6B/glove.6B.300d.txt\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0] ## The first entry is the word\n",
    "            coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('GloVe data loaded')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_features={}\n",
    "\n",
    "for x in range(len(texts)):\n",
    "    embedding_matrix = np.zeros((len(texts[x]), EMBEDDING_DIM))\n",
    "    for y in texts[x]:\n",
    "    \n",
    "        embedding_vector = embeddings_index.get(y)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[x]=embedding_vector\n",
    "    \n",
    "    image_to_features[labels[x]]= embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 300)\n"
     ]
    }
   ],
   "source": [
    "print(image_to_features['img15_480.jpg'].shape)\n",
    "#print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'holes': 1, 'put': 2, 'nof': 3, 'up': 4, 'patch': 5, 'signs': 6, 'the': 7, 'pan': 8, 'sgns': 9, 'hhe': 10, 'bcs': 11, 'fou reas': 12, 'f': 13, 'rocds': 14, 'rougt r': 15, 'psi': 16, 'rouh ross': 17, 'potch': 18, 'pou ross': 19}\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.47578001  0.36765    -0.68434    ... -0.64611    -0.19789\n",
      "  -0.57608998]\n",
      " [ 0.22228     0.19109    -0.34740999 ... -0.31937999 -0.058852\n",
      "   0.18498001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.5262     -0.11665    -0.38363999 ...  0.25213     0.23485\n",
      "   0.092975  ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] (20, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# labels = keras.utils.to_categorical(np.asarray(labels))\n",
    "\n",
    "print(word_index)\n",
    "print(embedding_matrix, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps we may need to take\n",
    "\n",
    "# (1) Run the code of that ocr repo to get the sentences\n",
    "# (2) With the sentences, calculate the longest sequence; MAX_SEQ_LENGTH of input should follow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BiLSTM without attention\n",
    "\n",
    "NUM_LSTM_UNITS = 128 #output dimension\n",
    "\n",
    "\n",
    "NUM_EMBEDDING_DIM = EMBEDDING_DIM\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),    #this is the first sentence\n",
    "    )\n",
    "\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ),   #this is the second\n",
    "    )\n",
    "\n",
    "\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "BiLSTM = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True))\n",
    "\n",
    "\n",
    "\n",
    "top_output = BiLSTM(top_embedded)\n",
    "\n",
    "bm_output = BiLSTM(bm_embedded)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "dense =  Dense(\n",
    "    units=290, \n",
    "    activation='sigmoid')\n",
    "\n",
    "predictions1 = dense(top_output)\n",
    "\n",
    "predictions2 = dense(bm_output)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=[predictions1, predictions2])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'])\n",
    "\n",
    "BATCH_SIZE = 72\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('../Phrase_Models_H5/big_merge.h5', save_best_only=True,  verbose=1, monitor='val_loss', mode='min')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
